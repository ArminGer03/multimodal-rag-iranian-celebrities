{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate kappa score"
      ],
      "metadata": {
        "id": "kO3Vl4xxKWx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "\n",
        "def extract_labels_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Extract labels from a Label Studio annotation file.\n",
        "    Returns a dictionary mapping item IDs to their labels.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    labels_1 = {}\n",
        "    labels_2 = {}\n",
        "\n",
        "    for item in data:\n",
        "        item_id = item['id']\n",
        "        annotations = item['annotations']  # Fixed the typo here\n",
        "\n",
        "        # Get the first annotation (assuming each item has at least one)\n",
        "        if annotations:\n",
        "            first_annotation = annotations[0]\n",
        "            result_1 = first_annotation['result']\n",
        "\n",
        "            # Find the choice label (assuming there's one choice field per annotation)\n",
        "            for r in result_1:\n",
        "                if r['type'] == 'choices':\n",
        "                    labels_1[item_id] = r['value']['choices'][0]\n",
        "                    break\n",
        "\n",
        "            second_annotation = annotations[1]\n",
        "            result_2 = first_annotation['result']\n",
        "            for r in result_2:\n",
        "                if r['type'] == 'choices':\n",
        "                    labels_2[item_id] = r['value']['choices'][0]\n",
        "                    break\n",
        "\n",
        "    return labels_1 , labels_2\n",
        "\n",
        "def calculate_kappa(file1_path):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's Kappa score between two Label Studio annotation files.\n",
        "    \"\"\"\n",
        "    # Extract labels from both files\n",
        "    labels1 , labels2 = extract_labels_from_file(file1_path)\n",
        "\n",
        "\n",
        "    # Find common items\n",
        "    common_ids = set(labels1.keys()) & set(labels2.keys())\n",
        "\n",
        "    if not common_ids:\n",
        "        raise ValueError(\"No common items found between the two files\")\n",
        "\n",
        "    # Prepare data for kappa calculation\n",
        "    y1 = []\n",
        "    y2 = []\n",
        "\n",
        "    for item_id in common_ids:\n",
        "        y1.append(labels1[item_id])\n",
        "        y2.append(labels2[item_id])\n",
        "\n",
        "    # Get all possible labels to handle single-label cases\n",
        "    all_labels = sorted(list(set(y1 + y2)))\n",
        "\n",
        "    # Calculate kappa score with explicit labels parameter\n",
        "    try:\n",
        "        kappa = cohen_kappa_score(y1, y2, labels=all_labels)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not calculate kappa score properly: {str(e)}\")\n",
        "        # Fallback calculation for edge cases\n",
        "        agreement = sum(1 for a, b in zip(y1, y2) if a == b) / len(y1)\n",
        "        expected_agreement = sum((y1.count(l)/len(y1)) * (y2.count(l)/len(y2)) for l in all_labels)\n",
        "        kappa = (agreement - expected_agreement) / (1 - expected_agreement) if expected_agreement < 1 else np.nan\n",
        "\n",
        "    return kappa\n",
        "\n"
      ],
      "metadata": {
        "id": "iDn7AEqm6WnW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    file1 = \"/content/output_with_lable.json\"\n",
        "\n",
        "    try:\n",
        "        kappa_score = calculate_kappa(file1)\n",
        "        print(f\"Cohen's Kappa Score: {kappa_score:.3f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZ0AITD_uAv",
        "outputId": "82180eb4-e903-4398-bed0-fa04157d69c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing statical parameters"
      ],
      "metadata": {
        "id": "HCD4ms6QJafA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading the data set"
      ],
      "metadata": {
        "id": "oHUnAb5EJm3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load your JSON data\n",
        "with open('/content/augmented-data.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Make sure it's a list of objects\n",
        "if isinstance(data, dict):\n",
        "    data = [data]\n"
      ],
      "metadata": {
        "id": "Ji7qRpfe_9CR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "computing number of all words"
      ],
      "metadata": {
        "id": "Mq_jfp8ZJmC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ۲. تعداد کل کلمات\n",
        "def count_words(value):\n",
        "    if isinstance(value, str):\n",
        "        return len(value.split())\n",
        "    elif isinstance(value, list):\n",
        "        return sum(count_words(item) for item in value)\n",
        "    elif isinstance(value, dict):\n",
        "        return sum(count_words(v) for v in value.values())\n",
        "    return 0"
      ],
      "metadata": {
        "id": "VL7Dx26dMSmy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculate average string length of each field"
      ],
      "metadata": {
        "id": "QAzjWI3iKCc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ۳. میانگین طول رشته‌ها برای هر ویژگی\n",
        "def get_string_lengths(value):\n",
        "    if isinstance(value, str):\n",
        "        return [len(value)]\n",
        "    elif isinstance(value, list):\n",
        "        lengths = []\n",
        "        for item in value:\n",
        "            lengths.extend(get_string_lengths(item))\n",
        "        return lengths\n",
        "    elif isinstance(value, dict):\n",
        "        lengths = []\n",
        "        for v in value.values():\n",
        "            lengths.extend(get_string_lengths(v))\n",
        "        return lengths\n",
        "    return []\n"
      ],
      "metadata": {
        "id": "cu-azqN6MWRN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute all parameter"
      ],
      "metadata": {
        "id": "6AfYGB5AKO_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_lengths_by_field = defaultdict(list)\n",
        "\n",
        "for obj in data:\n",
        "    for key, value in obj.items():\n",
        "        lengths = get_string_lengths(value)\n",
        "        string_lengths_by_field[key].extend(lengths)\n",
        "\n",
        "average_lengths_by_field = {\n",
        "    key: (sum(lengths) / len(lengths) if lengths else 0)\n",
        "    for key, lengths in string_lengths_by_field.items()\n",
        "}\n",
        "\n",
        "record_count = len(data)\n",
        "total_word_count = sum(count_words(obj) for obj in data)\n",
        "\n",
        "print(\"تعداد رکوردها:\", record_count)\n",
        "print(\"تعداد کل کلمات:\", total_word_count)\n",
        "print(\"میانگین طول رشته برای هر ویژگی:\")\n",
        "for key, avg_len in average_lengths_by_field.items():\n",
        "    if (key not in [\"sex\", \"birth\",\"death\",\"image\"]):\n",
        "      print(f\"  {key}: {avg_len:.2f}\")\n"
      ],
      "metadata": {
        "id": "AiZy_ivqKMt7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590b0b7a-3cb1-4f84-82be-db03f5281a31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تعداد رکوردها: 983\n",
            "تعداد کل کلمات: 32403\n",
            "میانگین طول رشته برای هر ویژگی:\n",
            "  name: 15.21\n",
            "  nick-names: 12.24\n",
            "  era: 7.31\n",
            "  occupation: 14.35\n",
            "  works: 28.82\n",
            "  events: 38.20\n"
          ]
        }
      ]
    }
  ]
}