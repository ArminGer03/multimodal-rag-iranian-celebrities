{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Crawling wikipedia for political figures\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 categories beginning with “سیاستمداران اهل ایران”:\n",
      "\n",
      "– سیاستمداران اهل ایران\n",
      "– سیاستمداران اهل ایران بر پایه استان\n",
      "– سیاستمداران اهل ایران بر پایه تبار یا قومیت\n",
      "– سیاستمداران اهل ایران بر پایه حزب\n",
      "– سیاستمداران اهل ایران بر پایه دوره\n",
      "– سیاستمداران اهل ایران بر پایه سده\n",
      "– سیاستمداران اهل ایران بر پایه شهر\n",
      "– سیاستمداران اهل ایران در دوره صفویه\n",
      "– سیاستمداران اهل ایران در دوره قاجار\n",
      "– سیاستمداران اهل ایران در دوره پهلوی\n"
     ]
    }
   ],
   "source": [
    "API_URL = \"https://fa.wikipedia.org/w/api.php\"\n",
    "\n",
    "PREFIX = \"سیاستمداران اهل ایران\"   # look for categories that begin with this\n",
    "\n",
    "paramssss = {\n",
    "    \"action\":     \"query\",\n",
    "    \"list\":       \"allcategories\",\n",
    "    \"acprefix\":   PREFIX,\n",
    "    \"aclimit\":    \"1000\",\n",
    "    \"format\":     \"json\"\n",
    "}\n",
    "\n",
    "resp = requests.get(API_URL, params=paramssss).json()\n",
    "cats = [c[\"*\"] for c in resp[\"query\"][\"allcategories\"]]\n",
    "\n",
    "print(f\"Found {len(cats)} categories beginning with “{PREFIX}”:\\n\")\n",
    "for c in cats:\n",
    "    print(\"–\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Running BFS on wikipedia pages\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3505 pages and 185 categories\n",
      "\n",
      "Pages:\n",
      "بحث کاربر:007media: https://fa.wikipedia.org/wiki/بحث_کاربر:007media\n",
      "بهمن آبادیان: https://fa.wikipedia.org/wiki/بهمن_آبادیان\n",
      "کامبیز آتابای: https://fa.wikipedia.org/wiki/کامبیز_آتابای\n",
      "عبدالقدیر آزاد: https://fa.wikipedia.org/wiki/عبدالقدیر_آزاد\n",
      "حسام‌الدین آشنا: https://fa.wikipedia.org/wiki/حسام‌الدین_آشنا\n",
      "فرج‌الله آصف: https://fa.wikipedia.org/wiki/فرج‌الله_آصف\n",
      "مهرآفاق آصف‌زاده: https://fa.wikipedia.org/wiki/مهرآفاق_آصف‌زاده\n",
      "کریم آصف: https://fa.wikipedia.org/wiki/کریم_آصف\n",
      "علی آقامحمدی: https://fa.wikipedia.org/wiki/علی_آقامحمدی\n",
      "یحیی آل اسحاق: https://fa.wikipedia.org/wiki/یحیی_آل_اسحاق\n",
      "\n",
      "Categories:\n",
      "تاریخ انتخاباتی سیاستمداران ایرانی: https://fa.wikipedia.org/wiki/رده:تاریخ_انتخاباتی_سیاستمداران_ایرانی\n",
      "رایزنان فرهنگی ایران: https://fa.wikipedia.org/wiki/رده:رایزنان_فرهنگی_ایران\n",
      "سران جنبش سبز: https://fa.wikipedia.org/wiki/رده:سران_جنبش_سبز\n",
      "سیاستمداران ترورشده اهل ایران: https://fa.wikipedia.org/wiki/رده:سیاستمداران_ترورشده_اهل_ایران\n",
      "سیاستمداران خودکشی‌کرده اهل ایران: https://fa.wikipedia.org/wiki/رده:سیاستمداران_خودکشی‌کرده_اهل_ایران\n",
      "سیاستمداران دارای ناتوانی جسمانی اهل ایران: https://fa.wikipedia.org/wiki/رده:سیاستمداران_دارای_ناتوانی_جسمانی_اهل_ایران\n",
      "سیاستمداران محکوم‌شده به جرم اهل ایران: https://fa.wikipedia.org/wiki/رده:سیاستمداران_محکوم‌شده_به_جرم_اهل_ایران\n",
      "سیاستمداران مخالف جمهوری اسلامی ایران: https://fa.wikipedia.org/wiki/رده:سیاستمداران_مخالف_جمهوری_اسلامی_ایران\n",
      "مقاله‌های خرد سیاستمدار ایرانی: https://fa.wikipedia.org/wiki/رده:مقاله‌های_خرد_سیاستمدار_ایرانی\n",
      "ورزشکاران-سیاستمداران اهل ایران: https://fa.wikipedia.org/wiki/رده:ورزشکاران-سیاستمداران_اهل_ایران\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Global visited set to track all visited URLs across function calls\n",
    "visited = set()\n",
    "\n",
    "def get_wikipedia_pages(category_url, max_depth=3, current_depth=0):\n",
    "    global visited\n",
    "    \n",
    "    if current_depth > max_depth or category_url in visited:\n",
    "        return [], []\n",
    "    \n",
    "    visited.add(category_url)\n",
    "    response = requests.get(category_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
    "        return [], []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pages = []\n",
    "    subcategories = []\n",
    "\n",
    "    # Extract pages\n",
    "    for div in soup.find_all(\"div\", class_=\"mw-category-group\"):\n",
    "        for li in div.find_all(\"li\"):\n",
    "            a_tag = li.find(\"a\")\n",
    "            if a_tag:\n",
    "                title = a_tag.get_text()\n",
    "                href = a_tag.get(\"href\")\n",
    "                if href and href.startswith(\"/wiki/\"):  # Only pick valid article links\n",
    "                    link = \"https://fa.wikipedia.org\" + href\n",
    "                    link = unquote(link)  # decode %XX sequences into normal text\n",
    "                    if \"رده:\" in link:\n",
    "                        subcategories.append((title, link))\n",
    "                    else:\n",
    "                        pages.append((title, link))\n",
    "    \n",
    "    # Recursively process subcategories\n",
    "    all_pages = pages.copy()\n",
    "    all_subcategories = subcategories.copy()\n",
    "    \n",
    "    for _, subcat_link in subcategories:\n",
    "        if subcat_link not in visited:\n",
    "            sub_pages, sub_categories = get_wikipedia_pages(subcat_link, max_depth, current_depth + 1)\n",
    "            all_pages.extend(sub_pages)\n",
    "            all_subcategories.extend(sub_categories)\n",
    "    \n",
    "    return all_pages, all_subcategories\n",
    "\n",
    "# Cherry-picked categories\n",
    "seed_categories = [\n",
    "    \"سیاستمداران اهل ایران\",\n",
    "    \"سیاستمداران اهل ایران بر پایه استان\",\n",
    "    \"سیاستمداران اهل ایران بر پایه تبار یا قومیت\",\n",
    "    \"سیاستمداران اهل ایران بر پایه حزب\",\n",
    "    \"سیاستمداران اهل ایران بر پایه دوره\",\n",
    "    \"سیاستمداران اهل ایران بر پایه سده\",\n",
    "    \"سیاستمداران اهل ایران بر پایه شهر\",\n",
    "    \"سیاستمداران اهل ایران در دوره صفویه\",\n",
    "    \"سیاستمداران اهل ایران در دوره قاجار\",\n",
    "    \"سیاستمداران اهل ایران در دوره پهلوی\",\n",
    "    \"افراد بر پایه گرایش سیاسی اهل ایران\",\n",
    "    \"افراد در دوره پهلوی\",\n",
    "    \"فرماندهان ارتش شاهنشاهی ایران\",\n",
    "    \"رده‌های با نام رئیس‌جمهورهای ایران\",\n",
    "    \"رده‌های با نام نخست‌وزیران ایران\"\n",
    "]\n",
    "\n",
    "pages = None\n",
    "categories = None\n",
    "\n",
    "for cat in seed_categories:\n",
    "    category_url = f'https://fa.wikipedia.org/wiki/رده:{cat}'\n",
    "\n",
    "    if pages is None:\n",
    "        pages_, categories_ = get_wikipedia_pages(category_url, max_depth=10)\n",
    "        pages = pages_\n",
    "        categories = categories_\n",
    "    else:\n",
    "        pages_, categories_ = get_wikipedia_pages(category_url, max_depth=10)\n",
    "        pages.extend(pages_)\n",
    "        categories.extend(categories_)\n",
    "\n",
    "    # Print results\n",
    "print(f\"Found {len(pages)} pages and {len(categories)} categories\")\n",
    "print(\"\\nPages:\")\n",
    "for title, link in pages[:10]:  # Print first 10 pages\n",
    "    print(f\"{title}: {link}\")\n",
    "\n",
    "print(\"\\nCategories:\")\n",
    "for title, link in categories[:10]:  # Print first 10 categories\n",
    "    print(f\"{title}: {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df2 = pd.DataFrame.from_records(pages).rename(columns={0: \"title\", 1: \"link\"}).set_index(\"title\").drop_duplicates()\n",
    "df2.to_csv(\"iran_political_figures.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Denoising the crawled data from non-person pages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wikidata human filter: 100%|██████████| 2513/2513 [35:10<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries:       2513\n",
      "Detected persons:    2344\n",
      "Filtered as noise:   169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ── Load your CSV ──\n",
    "csv_path = \"iran_political_figures.csv\"\n",
    "df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ── Prepare Wikidata API session ──\n",
    "WIKIDATA_API = \"https://www.wikidata.org/w/api.php\"\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"IranPolCrawler/1.0 (youremail@example.com)\"\n",
    "})\n",
    "\n",
    "def get_wd_entity_id(title: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Given a Persian Wikipedia title, return the corresponding Wikidata entity ID,\n",
    "    or None if not found.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"sites\": \"fawiki\",\n",
    "        \"titles\": title,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(WIKIDATA_API, params=params, timeout=10).json()\n",
    "    entities = r.get(\"entities\", {})\n",
    "    for eid, data in entities.items():\n",
    "        if eid != \"-1\":\n",
    "            return eid\n",
    "    return None\n",
    "\n",
    "def is_human_entity(eid: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a given Wikidata entity ID has P31 (instance of) Q5 (human).\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetclaims\",\n",
    "        \"entity\": eid,\n",
    "        \"property\": \"P31\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(WIKIDATA_API, params=params, timeout=10).json()\n",
    "    claims = r.get(\"claims\", {}).get(\"P31\", [])\n",
    "    for claim in claims:\n",
    "        dv = claim.get(\"mainsnak\", {}).get(\"datavalue\", {}).get(\"value\", {})\n",
    "        if dv.get(\"id\") == \"Q5\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# ── Run Wikidata-based human check ──\n",
    "results = []\n",
    "for title in tqdm(df[\"title\"], desc=\"Wikidata human filter\"):\n",
    "    eid = get_wd_entity_id(title)\n",
    "    if eid and is_human_entity(eid):\n",
    "        results.append(True)\n",
    "    else:\n",
    "        results.append(False)\n",
    "        # print(title)\n",
    "    time.sleep(0.1)  # gentle throttle\n",
    "\n",
    "df[\"is_person\"] = results\n",
    "\n",
    "# ── Split out persons vs. noise ──\n",
    "df_person = df[df[\"is_person\"]].drop(columns=[\"is_person\"])\n",
    "df_noise = df[~df[\"is_person\"]].drop(columns=[\"is_person\"])\n",
    "\n",
    "# ── Print summary ──\n",
    "print(f\"Total entries:       {len(df)}\")\n",
    "print(f\"Detected persons:    {len(df_person)}\")\n",
    "print(f\"Filtered as noise:   {len(df_noise)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Noise Entries                         title  \\\n",
      "0          بحث کاربر:007media   \n",
      "1                بهمن آبادیان   \n",
      "12               قربانعلی آهی   \n",
      "15          ابراهیم فرحبخشیان   \n",
      "16   ابراهیم‌خان یمین‌السلطنه   \n",
      "37            میرطاهر اردبیلی   \n",
      "44                اسحق فرهمند   \n",
      "45           اسدالله جوانمردی   \n",
      "60          اعظم‌الدوله زنگنه   \n",
      "101   ویکی‌پدیا:ابوالقاسم خان   \n",
      "\n",
      "                                                  link  \n",
      "0     https://fa.wikipedia.org/wiki/بحث_کاربر:007media  \n",
      "1           https://fa.wikipedia.org/wiki/بهمن_آبادیان  \n",
      "12          https://fa.wikipedia.org/wiki/قربانعلی_آهی  \n",
      "15     https://fa.wikipedia.org/wiki/ابراهیم_فرحبخشیان  \n",
      "16   https://fa.wikipedia.org/wiki/ابراهیم‌خان_یمین...  \n",
      "37       https://fa.wikipedia.org/wiki/میرطاهر_اردبیلی  \n",
      "44           https://fa.wikipedia.org/wiki/اسحق_فرهمند  \n",
      "45      https://fa.wikipedia.org/wiki/اسدالله_جوانمردی  \n",
      "60     https://fa.wikipedia.org/wiki/اعظم‌الدوله_زنگنه  \n",
      "101  https://fa.wikipedia.org/wiki/ویکی‌پدیا:ابوالق...  \n",
      "→ iran_political_figures_after_process.csv\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the noise entries for review\n",
    "print(\"Sample Noise Entries\", df_noise.head(10))\n",
    "\n",
    "df_person.to_csv(\"iran_political_figures_after_process.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"→ iran_political_figures_after_process.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Removing people with no infobox (indication of unreliable data)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking infobox:   0%|          | 0/2344 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking infobox: 100%|██████████| 2344/2344 [34:53<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2097 pages WITH infobox → with_infobox_politicians.csv\n",
      "✅ 247  pages WITHOUT infobox → no_infobox_politicians.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ── CONFIG ──\n",
    "INPUT_CSV  = \"iran_political_figures_after_process.csv\"\n",
    "OUT_WITH   = \"with_infobox_politicians.csv\"\n",
    "OUT_NO     = \"no_infobox_politicians.csv\"\n",
    "BASE_URL   = \"https://fa.wikipedia.org/wiki/\"\n",
    "\n",
    "# ── Session with retries ──\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"IranPolCrawler/1.0 (youremail@example.com)\",\n",
    "    \"Accept-Language\": \"fa\"\n",
    "})\n",
    "\n",
    "def page_has_infobox(title):\n",
    "    url = BASE_URL + urllib.parse.quote(title.replace(\" \", \"_\"))\n",
    "    try:\n",
    "        resp = session.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # network error or non-200 status\n",
    "        print(f\"[RequestError] {e} → {url}\")\n",
    "        return False\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    # first try the simple CSS selector\n",
    "    tbl = soup.select_one(\"table.infobox\")\n",
    "    if tbl:\n",
    "        return True\n",
    "\n",
    "    # fallback: any <table> whose class attribute contains \"infobox\"\n",
    "    tbl = soup.find(\"table\", attrs={\"class\": lambda c: c and \"infobox\" in c})\n",
    "    if tbl:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# ── Load titles ──\n",
    "df = pd.read_csv(INPUT_CSV, encoding=\"utf-8-sig\")\n",
    "\n",
    "with_infobox = []\n",
    "no_infobox   = []\n",
    "\n",
    "for title in tqdm(df[\"title\"], desc=\"Checking infobox\"):\n",
    "    has_box = page_has_infobox(title)\n",
    "    entry  = {\n",
    "        \"name\": title,\n",
    "        \"wikipedia_url\": BASE_URL + urllib.parse.quote(title.replace(\" \", \"_\"))\n",
    "    }\n",
    "    if has_box:\n",
    "        with_infobox.append(entry)\n",
    "    else:\n",
    "        no_infobox.append(entry)\n",
    "\n",
    "    time.sleep(0.2)  # be polite\n",
    "\n",
    "# ── Save CSVs ──\n",
    "pd.DataFrame(with_infobox).to_csv(OUT_WITH, index=False, encoding=\"utf-8-sig\")\n",
    "pd.DataFrame(no_infobox  ).to_csv(OUT_NO,   index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ {len(with_infobox)} pages WITH infobox → {OUT_WITH}\")\n",
    "print(f\"✅ {len(no_infobox)}  pages WITHOUT infobox → {OUT_NO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      name                                      wikipedia_url\n",
      "0                 مجید آهی  https://fa.wikipedia.org/wiki/%D9%85%D8%AC%DB%...\n",
      "1  ابوالفضل میرشمس شهشهانی  https://fa.wikipedia.org/wiki/%D8%A7%D8%A8%D9%...\n",
      "2          مالک اژدر شریفی  https://fa.wikipedia.org/wiki/%D9%85%D8%A7%D9%...\n",
      "3       میرزا حسین اصفهانی  https://fa.wikipedia.org/wiki/%D9%85%DB%8C%D8%...\n",
      "4            امیر نجم رشتی  https://fa.wikipedia.org/wiki/%D8%A7%D9%85%DB%...\n",
      "5        امیرحسین جهانشاهی  https://fa.wikipedia.org/wiki/%D8%A7%D9%85%DB%...\n",
      "6            امیرقلی امینی  https://fa.wikipedia.org/wiki/%D8%A7%D9%85%DB%...\n",
      "7                حسین اهری  https://fa.wikipedia.org/wiki/%D8%AD%D8%B3%DB%...\n",
      "8     بهاءالدین محمد جوینی  https://fa.wikipedia.org/wiki/%D8%A8%D9%87%D8%...\n",
      "9         حسن بن سهل سرخسی  https://fa.wikipedia.org/wiki/%D8%AD%D8%B3%D9%...\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(no_infobox).head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
