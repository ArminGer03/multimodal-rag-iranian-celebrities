{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b84aa67"
      },
      "source": [
        "# Task\n",
        "Convert the CSV data from the file 'Kamshad-Kooshan.csv' into a JSON format, where the 'images_clean' column values are transformed into URLs. The desired JSON structure for each record is: `{\"id\": \"...\", \"cleaned_bio\": \"...\", \"images\": [\"url1\", \"url2\", ...]}`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573201c3"
      },
      "source": [
        "## Load the csv data\n",
        "\n",
        "### Subtask:\n",
        "Load the CSV file into a pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30b25a72"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the CSV file into a pandas DataFrame as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dbd61c3",
        "outputId": "f2e741f1-29fa-4c98-9608-e5cd193b5cef"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/final_peaple_dataframe.csv')\n",
        "\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2111, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeca9da3"
      },
      "source": [
        "## Define a function to process each row\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes a row of the DataFrame and transforms it into the desired JSON structure. This function should also handle generating the image URLs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "689d8a20"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to transform a row of the DataFrame into the desired JSON structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8a2f4eb"
      },
      "source": [
        "# Task\n",
        "Convert a CSV file located at \"/Shared drives/shared/biography.csv\" to a JSON format, where the image paths in the CSV are converted to Google Drive shareable URLs. The output JSON should have the structure `{\"id\": \"...\", \"cleaned_bio\": \"...\", \"images\": [\"...\"]}` and be saved to \"/Shared drives/shared/biography.json\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1f3d714"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "### Subtask:\n",
        "Mount your Google Drive to access the shared folder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6d424c8"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to access the shared folder containing the CSV file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d566d0b",
        "outputId": "a244dfd3-8135-41ec-a0e5-30212f94b19a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 2: Install PyDrive ---\n",
        "!pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QkKCTU-M9tZ",
        "outputId": "2a0908ab-95a6-4b84-d8c2-becb195608d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (2.181.0)\n",
            "Requirement already satisfied: google-auth-httplib2 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (0.30.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client) (4.2.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9.1)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.8.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib) (3.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Build the Drive API service\n",
        "drive_service = build('drive', 'v3')\n"
      ],
      "metadata": {
        "id": "RN_qoU7vPAMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files_recursive(folder_id):\n",
        "    collected = {}\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        result = drive_service.files().list(\n",
        "            q=f\"'{folder_id}' in parents and trashed=false\",\n",
        "            pageSize=1000,\n",
        "            fields=\"nextPageToken, files(id, name, mimeType)\",\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        files = result.get('files', [])\n",
        "        for f in files:\n",
        "            if f['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                collected.update(list_files_recursive(f['id']))  # recurse into subfolder\n",
        "            else:\n",
        "                collected[f['name']] = f['id']\n",
        "\n",
        "        page_token = result.get('nextPageToken')\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "    return collected\n",
        "\n",
        "# Replace with your \"images_clean\" folder ID\n",
        "FOLDER_ID = \"1wuCk3Po7E9foHfJJbRYoIxUYWSLpuDDa\"\n",
        "\n",
        "file_id_map = list_files_recursive(FOLDER_ID)\n",
        "print(\"‚úÖ Found\", len(file_id_map), \"files\")\n",
        "print(list(file_id_map.items())[:10])  # preview"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkRlqrFNM5sV",
        "outputId": "c89ff73a-03a5-4890-aa9b-95ac77bdf327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found 6238 files\n",
            "[('250px-Ahmad_Shamlou_-_2_1.png', '1ZPoYJHr8z2EaRA7oAqndxvhOsNp88zl2'), ('250px-Rahi_moaieri.png', '1dU7a7xLYNPnhSvYYNF7iQrK3kgo-2OXn'), ('250px-Kamal_Khojandi_1.png', '1Nygio1QUx-vkznb5zGiuD7INO4ajxh66'), ('40px-Wikiquote-logo.svg_4.png', '1FsXgw5H7tCmrYdBkh6b1m5i_AydjJIpI'), ('250px-Jami_Rose_Garden.png', '1ouxqmpBIKlMiU21Pzmnr7q8uXgLR2ymk'), ('250px-%D9%85%D9%88%D9%84%D8%A7%D9%86%D8%A7_%D8%A7%D8%AB%D8%B1_%D8%AD%D8%B3%DB%8C%D9%86_%D8%A8%D9%87%D8%B2%D8%A7%D8%AF_%2_1.png', '1bE32Xr0_dOlJ0IH5FJ-z11wwgbE4j-f4'), ('250px-Sadi_in_a_Rose_garden.png', '19_jACZAQZiMJQJ3ulqbdVri7pIPaKrdX'), ('250px-Nizami_Rug_Crop.png', '1TWGGcSM6VwXjCpUpgUj6gcyRswDx7_T0'), ('330px-%D8%AA%D9%86%D8%AF%DB%8C%D8%B3_%D9%88_%D9%85%D9%82%D8%A8%D8%B1%D9%87_%D8%B9%D8%B7%D8%A7%D8%B1_%D9%86%DB%8C%D8%B4%D.png', '1keyyE-sz7hfIP8ohMcW29geMDJ8bUz6-'), ('250px-Khwaja_Abdullah_Ansari_portrait_1.png', '1kuNEccLUaIHaFM-T9ySmhEWPbmlLAgrS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "from tqdm import tqdm   # progress bar\n",
        "\n",
        "output = []\n",
        "valid_count = 0\n",
        "invalid_count = 0\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
        "    # Split by \"|\" to handle multiple images per row\n",
        "    raw_images = str(row[\"images_clean\"]).split(\"|\")\n",
        "\n",
        "    image_urls = []\n",
        "    for raw_img in raw_images:\n",
        "        filename = raw_img.strip().replace(\"images_clean\\\\\", \"\")\n",
        "\n",
        "        # Find file ID\n",
        "        file_id = file_id_map.get(filename)\n",
        "        image_url = f\"https://drive.google.com/uc?id={file_id}\" if file_id else None\n",
        "\n",
        "        # Check validity of the URL\n",
        "        if image_url:\n",
        "          image_urls.append(image_url)\n",
        "          valid_count += 1\n",
        "        else:\n",
        "            invalid_count += 1\n",
        "\n",
        "    record = {\n",
        "        \"id\": row[\"name\"],\n",
        "        \"cleaned_bio\": row[\"final_paragraph\"],\n",
        "        \"images\": image_urls  # ‚úÖ could have multiple valid images now\n",
        "    }\n",
        "    output.append(record)\n",
        "\n",
        "# Save JSON\n",
        "json_path = \"output.json\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ JSON saved to:\", json_path)\n",
        "print(f\"‚úîÔ∏è Valid image URLs: {valid_count}\")\n",
        "print(f\"‚ùå Invalid/missing image URLs: {invalid_count}\")\n",
        "print(f\"üìä Total rows processed: {len(df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mogu3BU4NJOZ",
        "outputId": "8fa9e1c9-43dc-45f3-fb31-77e1d3dc8640"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing rows: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2111/2111 [00:00<00:00, 10073.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON saved to: output.json\n",
            "‚úîÔ∏è Valid image URLs: 2227\n",
            "‚ùå Invalid/missing image URLs: 0\n",
            "üìä Total rows processed: 2111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install numpy==1.26.4\n",
        "!pip install hazm==0.7.0\n",
        "!pip install rapidfuzz\n",
        "!pip install \"nltk==3.6.7\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceBy05Meiwnv",
        "outputId": "582b36ee-0b7d-4c02-c622-959981ac4533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting hazm==0.7.0\n",
            "  Using cached hazm-0.7.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting nltk==3.3 (from hazm==0.7.0)\n",
            "  Using cached nltk-3.3.0.zip (1.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting libwapiti>=0.2.1 (from hazm==0.7.0)\n",
            "  Using cached libwapiti-0.2.1.tar.gz (233 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from nltk==3.3->hazm==0.7.0) (1.17.0)\n",
            "Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "\u001b[33m  DEPRECATION: Building 'nltk' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'nltk'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394581 sha256=7bd0ca7ee866b5ed33ab9f6393b8bee3e7fbb182e28c372f3ef9d3376c89ced5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/af/fc/2210fee1abf90c87b294772189d34242fb8c535f7b7d84fc6c\n",
            "\u001b[33m  DEPRECATION: Building 'libwapiti' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'libwapiti'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp312-cp312-linux_x86_64.whl size=179234 sha256=44c8681a80fcbb42058db8e0da4f8e4d31e2f80e0268c84b8625d161711f9fea\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/39/25/f2779acdff2da37909d983fe566910c27d34112d1255b52bc5\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "\u001b[2K  Attempting uninstall: nltk\n",
            "\u001b[2K    Found existing installation: nltk 3.9.1\n",
            "\u001b[2K    Uninstalling nltk-3.9.1:\n",
            "\u001b[2K      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [hazm]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading rapidfuzz-3.14.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.14.1\n",
            "Collecting nltk==3.6.7\n",
            "  Downloading nltk-3.6.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.6.7) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk==3.6.7) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk==3.6.7) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk==3.6.7) (4.67.1)\n",
            "Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.3\n",
            "    Uninstalling nltk-3.3:\n",
            "      Successfully uninstalled nltk-3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "hazm 0.7.0 requires nltk==3.3, but you have nltk 3.6.7 which is incompatible.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.6.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nltk-3.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "# Load datasets\n",
        "with open(\"/content/output.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data1 = json.load(f)\n",
        "\n",
        "with open(\"/content/updated_persons.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data2 = json.load(f)\n"
      ],
      "metadata": {
        "id": "CJzoMowqtMvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json\n",
        "import random\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "def normalize_name(name: str) -> str:\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    return \" \".join(name.strip().split())\n",
        "\n",
        "# Lookup for dataset2\n",
        "data2_lookup = {normalize_name(d[\"id\"]): d for d in data2}\n",
        "merged = []\n",
        "used_data2 = set()\n",
        "match_count = 0  # ‚úÖ count matches\n",
        "\n",
        "for rec1 in data1:\n",
        "    name1 = normalize_name(rec1[\"id\"])\n",
        "\n",
        "    if name1 in data2_lookup:\n",
        "        rec2 = data2_lookup[name1]\n",
        "        used_data2.add(name1)\n",
        "        match_count += 1  # ‚úÖ direct match found\n",
        "    else:\n",
        "        match = process.extractOne(name1, data2_lookup.keys(), scorer=fuzz.ratio)\n",
        "        if match and match[1] >= 90:\n",
        "            rec2 = data2_lookup[match[0]]\n",
        "            used_data2.add(match[0])\n",
        "            match_count += 1  # ‚úÖ fuzzy match found\n",
        "        else:\n",
        "            rec2 = None\n",
        "\n",
        "    if rec2:\n",
        "      if rec1.get(\"images\") and rec2.get(\"images\"):\n",
        "        chosen_images = random.choice([rec1[\"images\"], rec2[\"images\"]])\n",
        "      elif rec1.get(\"images\"):\n",
        "        chosen_images = rec1[\"images\"]\n",
        "      elif rec2.get(\"images\"):\n",
        "        chosen_images = rec2[\"images\"]\n",
        "      else:\n",
        "\n",
        "\n",
        "      chosen_images = []\n",
        "      merged.append({\n",
        "            \"id\": rec2[\"id\"],  # prefer dataset2 name\n",
        "            \"cleaned_bio\": rec2[\"cleaned_bio\"],  # prefer dataset2 bio\n",
        "            \"images\": combined_images\n",
        "        })\n",
        "    else:\n",
        "        merged.append(rec1)\n",
        "\n",
        "# Add remaining records from dataset2\n",
        "for key, rec2 in data2_lookup.items():\n",
        "    if key not in used_data2:\n",
        "        merged.append(rec2)\n",
        "\n",
        "# Save merged dataset\n",
        "with open(\"merged_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(merged, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Merged dataset saved to merged.json\")\n",
        "print(f\"üî¢ Total matches found: {match_count}\")\n",
        "print(f\"üìä Final merged dataset length: {len(merged)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58MejkXGilyx",
        "outputId": "ab26608d-3646-4567-dcf6-626960a54436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Merged dataset saved to merged.json\n",
            "üî¢ Total matches found: 1160\n",
            "üìä Final merged dataset length: 2467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(data2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_faIgXXhnDi8",
        "outputId": "4923493d-fa47-42be-932e-1061fffa80f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1549"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05SlhAhewcKi",
        "outputId": "6a5ca760-9438-4181-d6fe-57bd5d4ac145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2111"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "# Load the JSON data from the file\n",
        "json_path = \"/content/merged_data.json\" # Assuming the merged.json file is the input\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    json_data = json.load(f)\n",
        "len(json_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m_W46JjlBqi",
        "outputId": "7e9f9b59-35cc-49a1-9f47-58ec9e638569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2467"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0094f783",
        "outputId": "37d2be5e-b228-463d-9f92-ef7145bdd14c"
      },
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "\n",
        "# Define the output CSV file path\n",
        "csv_path = \"/content/output_merged.csv\"\n",
        "\n",
        "# Open the CSV file in write mode\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "    # Create a CSV writer object\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header row\n",
        "    csv_writer.writerow([\"id\", \"bio\", \"imag\"])\n",
        "\n",
        "    # Write the data rows\n",
        "    for record in json_data:\n",
        "        id_val = record.get(\"id\", \"\")\n",
        "        bio_val = record.get(\"cleaned_bio\", \"\")\n",
        "        images_val = record.get(\"images\", []) # Get the list of images\n",
        "\n",
        "        # Check for missing data and report\n",
        "        missing_info = []\n",
        "        if not id_val:\n",
        "            missing_info.append(\"id\")\n",
        "        if not bio_val:\n",
        "            missing_info.append(\"cleaned_bio\")\n",
        "        if not images_val:\n",
        "            missing_info.append(\"images\")\n",
        "\n",
        "        if missing_info:\n",
        "            print(f\"‚ùó Warning: Row with id='{id_val}' is missing: {', '.join(missing_info)}\")\n",
        "\n",
        "        # Join the list of images into a string representation suitable for CSV\n",
        "        imag_val = str(images_val)\n",
        "\n",
        "        csv_writer.writerow([id_val, bio_val, imag_val])\n",
        "\n",
        "print(f\"‚úÖ JSON data successfully converted to CSV and saved to {csv_path}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ JSON data successfully converted to CSV and saved to /content/output_merged.csv\n"
          ]
        }
      ]
    }
  ]
}