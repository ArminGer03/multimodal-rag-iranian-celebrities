{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11673478,"sourceType":"datasetVersion","datasetId":7326233},{"sourceId":11676494,"sourceType":"datasetVersion","datasetId":7328443}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport logging\nimport os\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:17:20.259986Z","iopub.execute_input":"2025-05-04T14:17:20.260330Z","iopub.status.idle":"2025-05-04T14:17:20.274015Z","shell.execute_reply.started":"2025-05-04T14:17:20.260303Z","shell.execute_reply":"2025-05-04T14:17:20.273141Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Part I: Cleaning data\n---","metadata":{}},{"cell_type":"code","source":"# ——— Logging setup ———\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# ——— Logging setup ———\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# ——— Configuration ———\n# Paths to the two JSON files generated earlier\nINPUT_FILES = [\n    '/kaggle/input/all-aft/all_figures_part1.json',\n    '/kaggle/input/all-aft/all_figures_part2.json'\n]\n# Output file for concatenated and cleaned data\nOUTPUT_FILE = 'all_figures_combined_clean.json'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:17:20.275640Z","iopub.execute_input":"2025-05-04T14:17:20.275907Z","iopub.status.idle":"2025-05-04T14:17:20.300611Z","shell.execute_reply.started":"2025-05-04T14:17:20.275886Z","shell.execute_reply":"2025-05-04T14:17:20.299744Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def load_and_concatenate(files):\n    \"\"\"\n    Load JSON records from each file in 'files' and concatenate into a single list.\n    \"\"\"\n    all_records = []\n    for filepath in files:\n        if not os.path.exists(filepath):\n            logging.warning(f\"File not found, skipping: {filepath}\")\n            continue\n        with open(filepath, 'r', encoding='utf-8') as f:\n            try:\n                records = json.load(f)\n                logging.info(f\"Loaded {len(records)} records from {filepath}\")\n                all_records.extend(records)\n            except json.JSONDecodeError as e:\n                logging.error(f\"Failed to parse JSON in {filepath}: {e}\")\n    logging.info(f\"Total concatenated records: {len(all_records)}\")\n    return all_records\n\n\ndef clean_data(records):\n    \"\"\"\n    Filter records by:\n      - 'occupation' is a non-empty list\n      - At least one of 'birth.location.province' or 'birth.location.city' is non-empty\n      - 'image' is a non-empty list\n    Returns the filtered list.\n    \"\"\"\n    clean_records = []\n    total = len(records)\n    for person in tqdm(records, desc=\"Cleaning records\", unit=\"record\"):\n        # 1) occupation must be a non-empty list\n        occ = person.get('occupation')\n        if not occ or not isinstance(occ, list):\n            continue\n        if len(occ) == 0:\n            continue\n\n        # 2) birth place: either province or city must be non-empty\n        birth = person.get('birth', {})\n        loc = birth.get('location', {})\n        province = loc.get('province', '')\n        city = loc.get('city', '')\n        if not (province or city):\n            continue\n\n        # 3) image must be a non-empty list\n        imgs = person.get('image')\n        if not imgs or not isinstance(imgs, list) or len(imgs) == 0:\n            continue\n\n        # 4) must have a birth date\n        birth_date = person.get('birth').get('date')\n\n        if not birth_date:\n            continue\n\n        clean_records.append(person)\n\n    logging.info(f\"Records kept: {len(clean_records)} / {total}\")\n    return clean_records\n\n\ndef save_records(records, output_file):\n    \"\"\"\n    Save the list of records to 'output_file' as pretty JSON.\n    \"\"\"\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(records, f, ensure_ascii=False, indent=2)\n    logging.info(f\"Saved cleaned data to '{output_file}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:17:20.302002Z","iopub.execute_input":"2025-05-04T14:17:20.302347Z","iopub.status.idle":"2025-05-04T14:17:20.323142Z","shell.execute_reply.started":"2025-05-04T14:17:20.302316Z","shell.execute_reply":"2025-05-04T14:17:20.322390Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Step 1: Load and concatenate\n    combined = load_and_concatenate(INPUT_FILES)\n\n    # Step 2: Clean combined records\n    cleaned = clean_data(combined)\n\n    # Step 3: Save final output\n    save_records(cleaned, OUTPUT_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:17:20.324258Z","iopub.execute_input":"2025-05-04T14:17:20.324739Z","iopub.status.idle":"2025-05-04T14:17:20.498315Z","shell.execute_reply.started":"2025-05-04T14:17:20.324704Z","shell.execute_reply":"2025-05-04T14:17:20.497423Z"}},"outputs":[{"name":"stderr","text":"Cleaning records: 100%|██████████| 2097/2097 [00:00<00:00, 849474.16record/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"print(f'Succesfully clean the data down to {len(cleaned)} figures!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:17:20.499824Z","iopub.execute_input":"2025-05-04T14:17:20.500070Z","iopub.status.idle":"2025-05-04T14:17:20.504425Z","shell.execute_reply.started":"2025-05-04T14:17:20.500041Z","shell.execute_reply":"2025-05-04T14:17:20.503448Z"}},"outputs":[{"name":"stdout","text":"Succesfully clean the data down to 983 figures!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Part II: Completion of unfilled values\n---","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport logging\nimport time\nimport re\nfrom tqdm import tqdm\nimport requests\nimport google.generativeai as genai\nfrom dateutil import parser as date_parser\n\n# ——— Logging setup ———\nlogging.basicConfig(level=logging.INFO,\n                    format='%(asctime)s %(levelname)s %(message)s')\n\n# ——— Configure LLM ———\ngenai.configure(api_key=\"AIzaSyCMekxl8NrGFutY8gT4n27buUrXZXuesTg\")\nmodel = genai.GenerativeModel(\"gemma-3-27b-it\")\n\n# ——— Retry logic for LLM calls ———\ndef retry_request(fn, *args, max_retries=5, backoff_factor=1.0, **kwargs):\n    backoff = backoff_factor\n    for attempt in range(max_retries):\n        try:\n            return fn(*args, **kwargs)\n        except Exception as e:\n            if attempt < max_retries - 1:\n                logging.warning(f\"Error '{e}', retrying in {backoff}s...\")\n                time.sleep(backoff)\n                backoff *= 2\n                continue\n            logging.error(f\"Final attempt failed: {e}\")\n            return None\n\n# ——— Date normalization helpers ———\ndef normalize_date(ds):\n    \"\"\"Try to parse into YYYY-MM-DD, else return original.\"\"\"\n    if not isinstance(ds, str) or not ds.strip():\n        return ds\n    try:\n        dt = date_parser.parse(ds, dayfirst=True, fuzzy=True)\n        return dt.strftime('%Y-%m-%d')\n    except Exception:\n        return ds\n\ndef needs_llm_date(ds):\n    \"\"\"Check if date string is not in ISO format YYYY-MM-DD.\"\"\"\n    return not bool(re.match(r'^\\d{4}-\\d{2}-\\d{2}$', ds))\n\n# ——— LLM date conversion ———\ndef generate_date_prompt(date_str):\n    instr = (\n        \"Convert the following date (which may be in Persian solar calendar or any format) \"\n        \"to Gregorian date in YYYY-MM-DD format. Reply only with the date between tags.\"\n    )\n    data = f\"<START OF DATA>{date_str}<END OF DATA>\"\n    return instr + \"\\n\" + data\n\n# ——— LLM helper ———\ndef ask_llm(prompt, is_json=True):\n    def _gen(): return model.generate_content(prompt)\n    resp = retry_request(_gen)\n    if not resp or not hasattr(resp, 'text'):\n        return None\n    raw = resp.text.strip()\n    start_tag, end_tag = '<START OF DATA>', '<END OF DATA>'\n    start, end = raw.find(start_tag), raw.find(end_tag)\n    if start != -1 and end != -1 and start < end:\n        content = raw[start + len(start_tag):end].strip()\n    else:\n        content = raw\n    content = re.sub(r'^```(?:json)?\\s*', '', content)\n    content = re.sub(r'\\s*```$', '', content).strip()\n    if is_json:\n        try:\n            return json.loads(content)\n        except json.JSONDecodeError:\n            logging.warning(\"JSON parse error, returning None.\")\n            return None\n    else:\n        return content\n\n# ——— English detection ———\ndef is_english_sentence(s):\n    try:\n        s.encode('utf-8').decode('ascii')\n        return True\n    except Exception:\n        return False\n\n# ——— Prompt generators ———\ndef generate_sex_prompt(datum):\n    instr = (\n        \"Predict the sex (MALE or FEMALE) for the following name. \"\n        \"Reply only with MALE or FEMALE between tags.\"\n    )\n    data = f\"<START OF DATA>{datum.get('name','')}<END OF DATA>\"\n    return instr + \"\\n\" + data, False\n\ndef generate_translation_prompt(datum, field):\n    serial = json.dumps(datum[field], ensure_ascii=False)\n    instr = (\n        \"Translate the following JSON value into Persian, preserving the JSON structure exactly. \"\n        \"Reply only with valid JSON between tags.\"\n    )\n    data = f\"<START OF DATA>{serial}<END OF DATA>\"\n    return instr + \"\\n\" + data, True\n\ndef generate_geo_prompt(datum, field):\n    instr = (\n        \"Given this JSON location with 'city', fill in missing 'province', 'latitude', and 'longitude'. \"\n        \"Output a JSON object with keys 'province' and 'coordinates' (with 'latitude' and 'longitude') between tags.\"\n    )\n    serial = json.dumps(datum[field]['location'], ensure_ascii=False)\n    data = f\"<START OF DATA>{serial}<END OF DATA>\"\n    return instr + \"\\n\" + data, True\n\ndef generate_text_translation_prompt(text):\n    instr = (\n        \"Translate the following text into Persian. Reply only with the translated text between tags.\"\n    )\n    data = f\"<START OF DATA>{text}<END OF DATA>\"\n    return instr + \"\\n\" + data, False\n\n# ——— Main augmentation & date normalization ———\nif __name__ == '__main__':\n    INPUT_FILE = '/kaggle/input/dataaaa/part1.json'\n    if not os.path.exists(INPUT_FILE):\n        logging.error(f\"File not found: {INPUT_FILE}\")\n        exit(1)\n\n    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n        records = json.load(f)\n\n    subset = records\n    logging.info(f\"Processing {len(subset)} records for augmentation and date normalization\")\n\n    for person in tqdm(subset, desc=\"Augmenting records\", unit=\"person\"):\n        # 1) Fill sex as lowercase\n        if not person.get('sex'):\n            prompt, is_json = generate_sex_prompt(person)\n            result = ask_llm(prompt, is_json)\n            if isinstance(result, str):\n                person['sex'] = result.strip().lower()\n\n        # 2) Translate fields\n        for field in ['era','occupation','works','events']:\n            val = person.get(field)\n            if isinstance(val,(str,list)) and (\n               (isinstance(val,str) and is_english_sentence(val)) or\n               (isinstance(val,list) and all(is_english_sentence(x) for x in val))\n            ):\n                prompt, is_json = generate_translation_prompt(person, field)\n                translated = ask_llm(prompt, is_json)\n                if translated is not None:\n                    person[field] = translated\n\n        # 3) Geo for birth and death\n        for field in ['birth','death']:\n            for location in ['location','tomb_location']:\n                loc = person.get(field,{}).get('location',{})\n                coords = loc.get('coordinates',{})\n                if loc.get('city') and (\n                   not loc.get('province') or\n                   not coords.get('latitude') or\n                   not coords.get('longitude')\n                ):\n                    prompt, is_json = generate_geo_prompt(person, field)\n                    filled = ask_llm(prompt, is_json)\n                    if isinstance(filled,dict):\n                        province = filled.get('province')\n                        if province:\n                            clean_prov = province.strip().strip('<>').strip()\n                            # translate province if in English\n                            if is_english_sentence(clean_prov):\n                                t_prompt, t_json = generate_text_translation_prompt(clean_prov)\n                                tprov = ask_llm(t_prompt, t_json)\n                                if isinstance(tprov, str):\n                                    clean_prov = tprov.strip().strip('<>').strip()\n                            person[field]['location']['province'] = clean_prov\n                        cf = filled.get('coordinates',{})\n                        if isinstance(cf,dict):\n                            for k,v in cf.items():\n                                if v: person[field]['location']['coordinates'][k]=v\n\n        # 4) Normalize & LLM-convert dates\n        for date_field in ['birth','death']:\n            ds = person.get(date_field,{}).get('date','')\n            if isinstance(ds,str) and ds.strip():\n                iso = normalize_date(ds)\n                if needs_llm_date(iso):\n                    dprompt = generate_date_prompt(ds)\n                    conv = ask_llm(dprompt, is_json=False)\n                    if isinstance(conv,str):\n                        conv_clean = conv.strip().strip('<>').strip()\n                        if re.match(r'^\\d{4}-\\d{2}-\\d{2}$',conv_clean):\n                            person[date_field]['date']=conv_clean\n                        else:\n                            person[date_field]['date']=iso if iso else ds\n                    else:\n                        person[date_field]['date']=iso if iso else ds\n                else:\n                    person[date_field]['date']=iso\n\n        # Save augmented JSON for all persons\n    OUTPUT_FILE = '/kaggle/working/all_figures_augmented_part1.json'\n    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n        json.dump(subset, f, ensure_ascii=False, indent=2)\n    logging.info(f\"Augmented data saved to {OUTPUT_FILE} ({len(subset)} records)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T19:03:13.415959Z","iopub.execute_input":"2025-05-04T19:03:13.416222Z","iopub.status.idle":"2025-05-04T20:02:27.372886Z","shell.execute_reply.started":"2025-05-04T19:03:13.416204Z","shell.execute_reply":"2025-05-04T20:02:27.371786Z"}},"outputs":[{"name":"stderr","text":"Augmenting records: 100%|██████████| 492/492 [59:10<00:00,  7.22s/person]  \n","output_type":"stream"}],"execution_count":1}]}