{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T10:15:41.946477Z",
     "iopub.status.busy": "2025-05-04T10:15:41.946158Z",
     "iopub.status.idle": "2025-05-04T10:15:46.485991Z",
     "shell.execute_reply": "2025-05-04T10:15:46.485331Z",
     "shell.execute_reply.started": "2025-05-04T10:15:41.946422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Downloading mwparserfromhell-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mwparserfromhell\n",
      "Successfully installed mwparserfromhell-0.6.6\n"
     ]
    }
   ],
   "source": [
    "!pip install mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T10:15:46.487748Z",
     "iopub.status.busy": "2025-05-04T10:15:46.487470Z",
     "iopub.status.idle": "2025-05-04T10:15:49.032096Z",
     "shell.execute_reply": "2025-05-04T10:15:49.031586Z",
     "shell.execute_reply.started": "2025-05-04T10:15:46.487727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from urllib.parse import quote\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import mwparserfromhell\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T10:15:49.033255Z",
     "iopub.status.busy": "2025-05-04T10:15:49.032839Z",
     "iopub.status.idle": "2025-05-04T10:15:49.039437Z",
     "shell.execute_reply": "2025-05-04T10:15:49.038794Z",
     "shell.execute_reply.started": "2025-05-04T10:15:49.033232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ——— Logging setup ———\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# ——— Configure LLM ———\n",
    "genai.configure(api_key=\"\")\n",
    "model = genai.GenerativeModel(\"gemma-3-27b-it\")\n",
    "\n",
    "# ——— Configuration ———\n",
    "CSV_PATH    = '/kaggle/input/part1fin/output_part1_fin.csv'\n",
    "URL_COL     = 'link'\n",
    "OUTPUT_FILE = f'all_figures_{int(time.time())}.json'\n",
    "\n",
    "SCHEMA_TEMPLATE = {\n",
    "    \"name\": \"\", \"sex\": \"\", \"nick-names\": [],\n",
    "    \"birth\": {\"date\": \"\", \"location\": {\"province\": \"\", \"city\": \"\", \"coordinates\": {\"latitude\": \"\", \"longitude\": \"\"}}},\n",
    "    \"death\": {\"date\": \"\", \"location\": {\"province\": \"\", \"city\": \"\", \"coordinates\": {\"latitude\": \"\", \"longitude\": \"\"}},\n",
    "              \"tomb_location\": {\"province\": \"\", \"city\": \"\", \"coordinates\": {\"latitude\": \"\", \"longitude\": \"\"}}},\n",
    "    \"era\": \"\", \"occupation\": [], \"works\": [], \"events\": [], \"image\": []\n",
    "}\n",
    "\n",
    "WIKIDATA_PROPS = {\n",
    "    'birth_date': 'P569', 'death_date': 'P570',\n",
    "    'birth_place': 'P19', 'death_place': 'P20',\n",
    "    'gender': 'P21', 'occupation': 'P106',\n",
    "    'notable_works': 'P800', 'nickname': 'P1449',\n",
    "    'resting_place': 'P119', 'era': 'P2348'\n",
    "}\n",
    "\n",
    "# ——— Shared session with headers ———\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    'User-Agent': 'MyDataExtractorBot/1.0 (https://yourdomain.example; mailto:you@example.com)'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T10:15:49.040685Z",
     "iopub.status.busy": "2025-05-04T10:15:49.040396Z",
     "iopub.status.idle": "2025-05-04T10:15:49.065711Z",
     "shell.execute_reply": "2025-05-04T10:15:49.065177Z",
     "shell.execute_reply.started": "2025-05-04T10:15:49.040654Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def retry_request(fn, *args, max_retries=5, backoff_factor=1.0, **kwargs):\n",
    "    \"\"\"Generic retry helper for HTTP calls and LLM generate_content.\"\"\"\n",
    "    backoff = backoff_factor\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except requests.HTTPError as e:\n",
    "            status = e.response.status_code\n",
    "            if status == 429 and attempt < max_retries:\n",
    "                logging.warning(f\"429 Too Many Requests – retrying in {backoff}s...\")\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            raise\n",
    "        except requests.RequestException as e:\n",
    "            if attempt < max_retries:\n",
    "                logging.warning(f\"Network error ({e}) – retrying in {backoff}s...\")\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # LLM or other transient errors\n",
    "            if 'rate limit' in str(e).lower() and attempt < max_retries:\n",
    "                logging.warning(f\"LLM rate limit – retrying in {backoff}s...\")\n",
    "                time.sleep(backoff)\n",
    "                backoff *= 2\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# Fetch summary + metadata\n",
    "\n",
    "def fetch_wiki_summary(title):\n",
    "    url = f'https://fa.wikipedia.org/api/rest_v1/page/summary/{title}'\n",
    "    def _get():\n",
    "        r = session.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    data = retry_request(_get)\n",
    "    return data.get('extract', ''), data\n",
    "\n",
    "# Fetch full Wikidata entity JSON\n",
    "def fetch_wikidata_entity(qid):\n",
    "    url = f'https://www.wikidata.org/wiki/Special:EntityData/{qid}.json'\n",
    "    def _get():\n",
    "        r = session.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    data = retry_request(_get)\n",
    "    return data.get('entities', {}).get(qid, {})\n",
    "\n",
    "# Fetch raw wikitext infobox\n",
    "\n",
    "def fetch_infobox(title):\n",
    "    api_url = 'https://fa.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "        'action': 'query', 'titles': title,\n",
    "        'prop': 'revisions', 'rvslots': 'main', 'rvprop': 'content',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    def _get():\n",
    "        r = session.get(api_url, params=params, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    pages = retry_request(_get).get('query', {}).get('pages', {})\n",
    "    wikitext = next(iter(pages.values()), {}) \\\n",
    "               .get('revisions', [{}])[0] \\\n",
    "               .get('slots', {}).get('main', {}).get('*', '')\n",
    "    if not wikitext:\n",
    "        return {}\n",
    "    wikicode = mwparserfromhell.parse(wikitext)\n",
    "    for tmpl in wikicode.filter_templates():\n",
    "        nm = tmpl.name.strip().lower()\n",
    "        if 'جعبه اطلاعات' in nm or nm.startswith('infobox'):\n",
    "            return {str(p.name).strip(): str(p.value).strip() for p in tmpl.params}\n",
    "    return {}\n",
    "\n",
    "# Parse Wikidata claims\n",
    "\n",
    "def parse_wikidata(entity):\n",
    "    result = {}\n",
    "    claims = entity.get('claims', {})\n",
    "    for key, pid in WIKIDATA_PROPS.items():\n",
    "        vals = []\n",
    "        for claim in claims.get(pid, []):\n",
    "            snak = claim.get('mainsnak', {})\n",
    "            dv = snak.get('datavalue', {})\n",
    "            if not dv: continue\n",
    "            dtype = dv.get('type')\n",
    "            if dtype == 'string':\n",
    "                v = dv['value']\n",
    "            elif dtype == 'time':\n",
    "                t = dv['value'].get('time', '')\n",
    "                v = t.split('T')[0].lstrip('+') if t else ''\n",
    "            elif dtype == 'wikibase-entityid':\n",
    "                q = dv['value']['id']\n",
    "                lbl_ent = fetch_wikidata_entity(q)\n",
    "                v = lbl_ent.get('labels', {}).get('fa', {}).get('value') \\\n",
    "                    or lbl_ent.get('labels', {}).get('en', {}).get('value', '')\n",
    "            else:\n",
    "                continue\n",
    "            if v:\n",
    "                vals.append(v)\n",
    "        # single vs multi\n",
    "        if key in ('birth_date', 'death_date'):\n",
    "            result[key] = vals[0] if vals else ''\n",
    "        else:\n",
    "            result[key] = vals if vals else []\n",
    "    # images (P18)\n",
    "    images = []\n",
    "    for claim in claims.get('P18', []):\n",
    "        pic = claim.get('mainsnak', {}).get('datavalue', {}).get('value','')\n",
    "        if pic:\n",
    "            fn = quote(pic.replace(' ', '_'), safe='')\n",
    "            images.append(f'https://commons.wikimedia.org/wiki/Special:FilePath/{fn}')\n",
    "    if images:\n",
    "        result['image'] = images\n",
    "    return result\n",
    "\n",
    "# LLM-driven JSON fill\n",
    "\n",
    "def llm_fill(schema, summary):\n",
    "    prompt = f\"\"\"\n",
    "You are a data extractor. Given the following Wikipedia summary in Farsi,\n",
    "fill in this JSON schema. Leave fields empty if unknown.\n",
    "\n",
    "Summary:\n",
    "{summary}\n",
    "\n",
    "JSON schema:\n",
    "{json.dumps(schema, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Respond with only the completed JSON object.\n",
    "\"\"\"\n",
    "    def _gen():\n",
    "        return model.generate_content(prompt)\n",
    "    resp = retry_request(_gen)\n",
    "    raw = resp.text.strip()\n",
    "    inner = raw.split(\"```\")[1].lstrip(\"json\").strip() if raw.startswith(\"```\") else raw\n",
    "    try:\n",
    "        return json.loads(inner)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.warning(\"LLM output not valid JSON, using empty schema fallback.\")\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-04T10:15:49.067376Z",
     "iopub.status.busy": "2025-05-04T10:15:49.067157Z",
     "iopub.status.idle": "2025-05-04T12:22:00.911854Z",
     "shell.execute_reply": "2025-05-04T12:22:00.911320Z",
     "shell.execute_reply.started": "2025-05-04T10:15:49.067359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing figures: 100%|██████████| 1048/1048 [2:06:11<00:00,  7.22s/it] \n"
     ]
    }
   ],
   "source": [
    "# Main processing loop\n",
    "\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as fp:\n",
    "        fp.write('[\\n')\n",
    "        first = True\n",
    "\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing figures\"):\n",
    "            title = row[URL_COL].rsplit('/', 1)[-1]\n",
    "            try:\n",
    "                # Fetch data\n",
    "                summary, meta = fetch_wiki_summary(title)\n",
    "                qid = meta.get('wikibase_item', '')\n",
    "                person = json.loads(json.dumps(SCHEMA_TEMPLATE))\n",
    "\n",
    "                # 1) LLM-driven fill\n",
    "                llm_data = llm_fill(person, summary)\n",
    "                for k in person:\n",
    "                    person[k] = llm_data.get(k, person[k])\n",
    "\n",
    "                # 2) Wikidata parsing\n",
    "                wd = parse_wikidata(fetch_wikidata_entity(qid)) if qid else {}\n",
    "                # 3) Infobox fallback\n",
    "                infobox = fetch_infobox(title)\n",
    "\n",
    "                # Merge: LLM → Wikidata → Infobox\n",
    "                # Image\n",
    "                if not person['image']:\n",
    "                    if wd.get('image'):\n",
    "                        person['image'] = wd['image']\n",
    "                    elif meta.get('originalimage', {}).get('source'):\n",
    "                        person['image'] = [meta['originalimage']['source']]\n",
    "                    elif meta.get('thumbnail', {}).get('source'):\n",
    "                        person['image'] = [meta['thumbnail']['source']]\n",
    "                # Birth date & place\n",
    "                if not person['birth']['date']:\n",
    "                    person['birth']['date'] = wd.get('birth_date', '') or infobox.get('زادروز', '')\n",
    "                if not person['birth']['location']['city']:\n",
    "                    bp = wd.get('birth_place', [])\n",
    "                    person['birth']['location']['city'] = bp[0] if bp else infobox.get('زادگاه', '')\n",
    "                # Death date & place\n",
    "                if not person['death']['date']:\n",
    "                    person['death']['date'] = wd.get('death_date', '') or infobox.get('درگذشت', '')\n",
    "                if not person['death']['location']['city']:\n",
    "                    dp = wd.get('death_place', [])\n",
    "                    person['death']['location']['city'] = dp[0] if dp else infobox.get('محل درگذشت', '')\n",
    "                # Tomb location\n",
    "                if not person['death']['tomb_location']['city']:\n",
    "                    rp = wd.get('resting_place', [])\n",
    "                    person['death']['tomb_location']['city'] = rp[0] if rp else infobox.get('محل دفن', '')\n",
    "                # Occupation\n",
    "                if not person['occupation']:\n",
    "                    person['occupation'] = wd.get('occupation', []) or (\n",
    "                        [o.strip() for o in infobox.get('حرفه', '').split(',')] if infobox.get('حرفه') else []\n",
    "                    )\n",
    "                # Works\n",
    "                if not person['works']:\n",
    "                    person['works'] = wd.get('notable_works', []) or (\n",
    "                        [w.strip() for w in infobox.get('آثار', '').split(',')] if infobox.get('آثار') else []\n",
    "                    )\n",
    "                # Nicknames\n",
    "                if not person['nick-names']:\n",
    "                    person['nick-names'] = wd.get('nickname', []) or (\n",
    "                        [n.strip() for n in infobox.get('لقب', '').split(',')] if infobox.get('لقب') else []\n",
    "                    )\n",
    "                # Era\n",
    "                if not person['era']:\n",
    "                    person['era'] = wd.get('era', '') or infobox.get('دوران', '')\n",
    "\n",
    "                # Write output\n",
    "                if not first:\n",
    "                    fp.write(',\\n')\n",
    "                fp.write(json.dumps(person, ensure_ascii=False))\n",
    "                first = False\n",
    "\n",
    "                # Throttle\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed on {title}: {e}\", exc_info=True)\n",
    "                continue\n",
    "\n",
    "        fp.write('\\n]')\n",
    "    logging.info(f\"Completed: processed {len(df)} entries -> {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7316149,
     "sourceId": 11658345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7323332,
     "sourceId": 11669231,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7324146,
     "sourceId": 11670575,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
